{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nepali Pretrained Tokenizers examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy'] = 'http://192.41.170.23:3128'\n",
    "os.environ['https_proxy'] = 'http://192.41.170.23:3128'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bold(text):\n",
    "    return \"\\033[1m\" + text + \"\\033[0m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/suyogyat/research/.RNN/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer IDs\n",
    "# BERT_SHUSHANT = 'Shushant/nepaliBERT'\n",
    "# BERT_NOWALAB = 'nowalab/nepali-bert-npvec1'\n",
    "NEP_BERT = 'Rajan/NepaliBERT'\n",
    "NEP_ROBERTA = 'amitness/nepbert'\n",
    "NEP_DISTILBERT = 'Sakonii/distilbert-base-nepali'\n",
    "NEP_DEBERTA = 'Sakonii/deberta-base-nepali'\n",
    "XLM_ROBERTA = 'xlm-roberta-base'\n",
    "M_BERT = 'bert-base-multilingual-uncased'\n",
    "HIN_ROBERTA = 'flax-community/roberta-hindi'\n",
    "\n",
    "tokenizer_ids = [NEP_BERT, NEP_ROBERTA, NEP_DISTILBERT, NEP_DEBERTA, XLM_ROBERTA, M_BERT, HIN_ROBERTA]\n",
    "wp_tokenizer_ids = [NEP_BERT, M_BERT]\n",
    "sp_tokenizer_ids = [NEP_DISTILBERT, NEP_DEBERTA, XLM_ROBERTA]\n",
    "bp_tokenizer_ids = [NEP_ROBERTA, HIN_ROBERTA]\n",
    "\n",
    "# List of Sentences\n",
    "sentences = [\n",
    "    'नेपाल आफ्नै संस्कृतिका कारण विश्वमा सबैतिर चिनिएको हो ।', \n",
    "    'स्वास्थ्य तथा जनसंख्या मन्त्रालयले गत असार ९ गतेदेखि १५ गतेसम्म खोप लगाएका बालबालिकालाई आजदेखि दोस्रो मात्रा लगाउन थालेको हो ।', \n",
    "    'युरोपमा केही दिनयताको उच्च तापक्रमसँगै फैलिएको डढेलोका कारण जनजीवन अस्तव्यस्त बनेको छ ।',\n",
    "    'काठमाडौं महानगरपालिकाले साउन १ गतेदेखि कुहिने र नकुहिने फोहोरलाई छुट्टाछुट्टै दिनमा संकलन गर्ने भएको छ ।',\n",
    "    'काठमाडौंको नागार्जुनमा चितुवाको आक्रमणबाट पाँच जना घाइते भएका छन् ।',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Rajan/NepaliBERT ...\n",
      "Loading amitness/nepbert ...\n",
      "Loading Sakonii/distilbert-base-nepali ...\n",
      "Loading Sakonii/deberta-base-nepali ...\n",
      "Loading xlm-roberta-base ...\n",
      "Loading bert-base-multilingual-uncased ...\n",
      "Loading flax-community/roberta-hindi ...\n"
     ]
    }
   ],
   "source": [
    "tokenizers = []\n",
    "for id in tokenizer_ids:\n",
    "    print('Loading', id, '...')\n",
    "    tokenizers.append(AutoTokenizer.from_pretrained(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "wp_tokenizers = [tokenizers[0], tokenizers[5]]\n",
    "sp_tokenizers = [tokenizers[2], tokenizers[3], tokenizers[4]]\n",
    "bp_tokenizers = [tokenizers[1], tokenizers[6]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(text, tokenizers=tokenizers):\n",
    "    for idx, tokenizer in enumerate(tokenizers):\n",
    "        print(bold(tokenizer.name_or_path))\n",
    "        print(' '.join(tokenizer.tokenize(text)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='Rajan/NepaliBERT', vocab_size=50000, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "\n",
      "PreTrainedTokenizerFast(name_or_path='amitness/nepbert', vocab_size=52000, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)})\n",
      "\n",
      "PreTrainedTokenizerFast(name_or_path='Sakonii/distilbert-base-nepali', vocab_size=24581, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)})\n",
      "\n",
      "PreTrainedTokenizerFast(name_or_path='Sakonii/deberta-base-nepali', vocab_size=24581, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)})\n",
      "\n",
      "PreTrainedTokenizerFast(name_or_path='xlm-roberta-base', vocab_size=250002, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)})\n",
      "\n",
      "PreTrainedTokenizerFast(name_or_path='bert-base-multilingual-uncased', vocab_size=105879, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "\n",
      "PreTrainedTokenizerFast(name_or_path='flax-community/roberta-hindi', vocab_size=50265, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)})\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(f'{tokenizer}\\n') for tokenizer in tokenizers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: नेपाल आफ्नै संस्कृतिका कारण विश्वमा सबैतिर चिनिएको हो ।\n",
      "\n",
      "\u001b[1mRajan/NepaliBERT\u001b[0m\n",
      "नपा ##ल आफ ##न सस ##कति ##का कारण विश ##वमा सब ##तिर चिनिएको हो ।\n",
      "\n",
      "\u001b[1mamitness/nepbert\u001b[0m\n",
      "à¤¨ à¥ĩ à¤ª à¤¾ à¤² Ġà¤Ĩà¤« à¥į à¤¨ à¥Ī Ġà¤¸ à¤Ĥ à¤¸ à¥į à¤ķ à¥ĥ à¤¤ à¤¿ à¤ķ à¤¾ Ġà¤ķ à¤¾ à¤°à¤£ Ġà¤µ à¤¿ à¤¶ à¥į à¤µà¤® à¤¾ Ġà¤¸à¤¬ à¥Ī à¤¤ à¤¿ à¤° Ġà¤ļ à¤¿ à¤¨ à¤¿ à¤ıà¤ķ à¥ĭ Ġà¤¹ à¥ĭ Ġà¥¤\n",
      "\n",
      "\u001b[1mSakonii/distilbert-base-nepali\u001b[0m\n",
      "▁नेपाल ▁आफ्नै ▁संस्कृति का ▁कारण ▁विश्वमा ▁सबैतिर ▁चिनिए को ▁हो ▁।\n",
      "\n",
      "\u001b[1mSakonii/deberta-base-nepali\u001b[0m\n",
      "▁नेपाल ▁आफ्नै ▁संस्कृति का ▁कारण ▁विश्वमा ▁सबैतिर ▁चिनिए को ▁हो ▁।\n",
      "\n",
      "\u001b[1mxlm-roberta-base\u001b[0m\n",
      "▁नेपाल ▁आफ्नै ▁संस्कृति का ▁कारण ▁विश्व मा ▁सबै तिर ▁चिन िएको ▁हो ▁।\n",
      "\n",
      "\u001b[1mbert-base-multilingual-uncased\u001b[0m\n",
      "नपाल आ ##फ ##न ससकत ##िका कारण विशव ##मा सब ##ति ##र च ##िन ##िएको हो ।\n",
      "\n",
      "\u001b[1mflax-community/roberta-hindi\u001b[0m\n",
      "à¤¨ à¥ĩ à¤ª à¤ ¾ à¤² Ġà¤Ĩà¤« à¥į à¤¨ à¥Ī Ġà¤¸ à¤Ĥ à¤¸ à¥į à¤ķ à¥ĥ à¤¤ à¤¿ à¤ķ à¤ ¾ Ġà¤ķ à¤ ¾ à¤°à¤£ Ġà¤µ à¤¿ à¤¶ à¥į à¤µà¤® à¤ ¾ Ġà¤¸à¤¬ à¥Ī à¤¤ à¤¿ à¤° Ġà¤ļ à¤¿ à¤¨ à¤¿ à¤ıà¤ķ à¥ĭ Ġà¤¹ à¥ĭ Ġà¥ ¤\n",
      "\n",
      "======================================================================================================================================================\n",
      "Sentence 2: स्वास्थ्य तथा जनसंख्या मन्त्रालयले गत असार ९ गतेदेखि १५ गतेसम्म खोप लगाएका बालबालिकालाई आजदेखि दोस्रो मात्रा लगाउन थालेको हो ।\n",
      "\n",
      "\u001b[1mRajan/NepaliBERT\u001b[0m\n",
      "सवा ##स ##थ ##य तथा जन ##स ##ख ##या मन ##तर ##ालय ##ल गत असार [UNK] गत ##द ##खि [UNK] गत ##सम ##म खोप लगाएका बालबालिकालाई आज ##द ##खि दोस ##रो मात ##रा लगाउन थाल ##को हो ।\n",
      "\n",
      "\u001b[1mamitness/nepbert\u001b[0m\n",
      "à¤¸ à¥į à¤µ à¤¾ à¤¸ à¥į à¤¥ à¥į à¤¯ Ġà¤¤à¤¥ à¤¾ Ġà¤ľà¤¨à¤¸ à¤Ĥ à¤ĸ à¥į à¤¯ à¤¾ Ġà¤®à¤¨ à¥į à¤¤ à¥į à¤° à¤¾ à¤²à¤¯à¤² à¥ĩ Ġà¤Ĺà¤¤ Ġà¤ħà¤¸ à¤¾ à¤° Ġà¥¯ Ġà¤Ĺà¤¤ à¥ĩ à¤¦ à¥ĩ à¤ĸ à¤¿ Ġà¥§à¥« Ġà¤Ĺà¤¤ à¥ĩ à¤¸à¤® à¥į à¤® Ġà¤ĸ à¥ĭ à¤ª Ġà¤²à¤Ĺ à¤¾ à¤ıà¤ķ à¤¾ Ġà¤¬ à¤¾ à¤²à¤¬ à¤¾ à¤² à¤¿ à¤ķ à¤¾ à¤² à¤¾ à¤Ī Ġà¤Ĩà¤ľà¤¦ à¥ĩ à¤ĸ à¤¿ Ġà¤¦ à¥ĭ à¤¸ à¥į à¤° à¥ĭ Ġà¤® à¤¾ à¤¤ à¥į à¤° à¤¾ Ġà¤²à¤Ĺ à¤¾ à¤īà¤¨ Ġà¤¥ à¤¾ à¤² à¥ĩ à¤ķ à¥ĭ Ġà¤¹ à¥ĭ Ġà¥¤\n",
      "\n",
      "\u001b[1mSakonii/distilbert-base-nepali\u001b[0m\n",
      "▁स्वास्थ्य ▁तथा ▁जनसंख्या ▁मन्त्रालयले ▁गत ▁असार ▁९ ▁गतेदेखि ▁१५ ▁गतेसम्म ▁खोप ▁लगाएका ▁बालबालिकालाई ▁आजदेखि ▁दोस्रो ▁मात्रा ▁लगाउन ▁थालेको ▁हो ▁।\n",
      "\n",
      "\u001b[1mSakonii/deberta-base-nepali\u001b[0m\n",
      "▁स्वास्थ्य ▁तथा ▁जनसंख्या ▁मन्त्रालयले ▁गत ▁असार ▁९ ▁गतेदेखि ▁१५ ▁गतेसम्म ▁खोप ▁लगाएका ▁बालबालिकालाई ▁आजदेखि ▁दोस्रो ▁मात्रा ▁लगाउन ▁थालेको ▁हो ▁।\n",
      "\n",
      "\u001b[1mxlm-roberta-base\u001b[0m\n",
      "▁स्वास्थ्य ▁तथा ▁जनसंख्या ▁मन्त्रालयले ▁गत ▁असार ▁९ ▁गतेदेखि ▁१५ ▁गतेसम्म ▁खो प ▁लगाए का ▁बालबालिका लाई ▁आज देखि ▁दोस्रो ▁मात्रा ▁लगाउन ▁थालेको ▁हो ▁।\n",
      "\n",
      "\u001b[1mbert-base-multilingual-uncased\u001b[0m\n",
      "सवा ##स ##थय तथा जनसखया मन ##तर ##ालय ##ल ग ##त अस ##ार ९ ग ##त ##द ##ख ##ि १५ ग ##त ##सम ##म ख ##ो ##प लगा ##एका ब ##ाल ##बा ##लि ##काल ##ाई आज ##द ##ख ##ि दो ##सर ##ो मातरा लगा ##उन था ##लक ##ो हो ।\n",
      "\n",
      "\u001b[1mflax-community/roberta-hindi\u001b[0m\n",
      "à¤¸ à¥į à¤µ à¤ ¾ à¤¸ à¥į à¤¥ à¥į à¤¯ Ġà¤¤à¤¥ à¤ ¾ Ġà¤ľà¤¨à¤¸ à¤Ĥ à¤ĸ à¥į à¤¯ à¤ ¾ Ġà¤®à¤¨ à¥į à¤¤ à¥į à¤° à¤ ¾ à¤²à¤¯à¤² à¥ĩ Ġà¤Ĺà¤¤ Ġà¤ħà¤¸ à¤ ¾ à¤° Ġà¥¯ Ġà¤Ĺà¤¤ à¥ĩ à¤¦ à¥ĩ à¤ĸ à¤¿ Ġà¥§à¥« Ġà¤Ĺà¤¤ à¥ĩ à¤¸à¤® à¥į à¤® Ġà¤ĸ à¥ĭ à¤ª Ġà¤²à¤Ĺ à¤ ¾ à¤ıà¤ķ à¤ ¾ Ġà¤¬ à¤ ¾ à¤²à¤¬ à¤ ¾ à¤² à¤¿ à¤ķ à¤ ¾ à¤² à¤ ¾ à¤Ī Ġà¤Ĩà¤ľà¤¦ à¥ĩ à¤ĸ à¤¿ Ġà¤¦ à¥ĭ à¤¸ à¥į à¤° à¥ĭ Ġà¤® à¤ ¾ à¤¤ à¥į à¤° à¤ ¾ Ġà¤²à¤Ĺ à¤ ¾ à¤īà¤¨ Ġà¤¥ à¤ ¾ à¤² à¥ĩ à¤ķ à¥ĭ Ġà¤¹ à¥ĭ Ġà¥ ¤\n",
      "\n",
      "======================================================================================================================================================\n",
      "Sentence 3: युरोपमा केही दिनयताको उच्च तापक्रमसँगै फैलिएको डढेलोका कारण जनजीवन अस्तव्यस्त बनेको छ ।\n",
      "\n",
      "\u001b[1mRajan/NepaliBERT\u001b[0m\n",
      "य ##रोप ##मा कही दिनयता ##को उच ##च ताप ##कर ##मस ##ग फल ##िएको डढ ##लोक ##ा कारण जनजीवन अस ##त ##व ##यस ##त बन ##को छ ।\n",
      "\n",
      "\u001b[1mamitness/nepbert\u001b[0m\n",
      "à¤¯ à¥ģ à¤° à¥ĭ à¤ªà¤® à¤¾ Ġà¤ķ à¥ĩ à¤¹ à¥Ģ Ġà¤¦ à¤¿ à¤¨à¤¯à¤¤ à¤¾ à¤ķ à¥ĭ Ġà¤īà¤ļ à¥į à¤ļ Ġà¤¤ à¤¾ à¤ªà¤ķ à¥į à¤°à¤®à¤¸ à¤ģ à¤Ĺ à¥Ī Ġà¤« à¥Ī à¤² à¤¿ à¤ıà¤ķ à¥ĭ Ġà¤¡à¤¢ à¥ĩ à¤² à¥ĭ à¤ķ à¤¾ Ġà¤ķ à¤¾ à¤°à¤£ Ġà¤ľà¤¨à¤ľ à¥Ģ à¤µà¤¨ Ġà¤ħà¤¸ à¥į à¤¤à¤µ à¥į à¤¯à¤¸ à¥į à¤¤ Ġà¤¬à¤¨ à¥ĩ à¤ķ à¥ĭ Ġà¤Ľ Ġà¥¤\n",
      "\n",
      "\u001b[1mSakonii/distilbert-base-nepali\u001b[0m\n",
      "▁युरोप मा ▁केही ▁दिन यता को ▁उच्च ▁तापक्रम सँगै ▁फैलिएको ▁डढेलो का ▁कारण ▁जनजीवन ▁अस्तव्यस्त ▁बनेको ▁छ ▁।\n",
      "\n",
      "\u001b[1mSakonii/deberta-base-nepali\u001b[0m\n",
      "▁युरोप मा ▁केही ▁दिन यता को ▁उच्च ▁तापक्रम सँगै ▁फैलिएको ▁डढेलो का ▁कारण ▁जनजीवन ▁अस्तव्यस्त ▁बनेको ▁छ ▁।\n",
      "\n",
      "\u001b[1mxlm-roberta-base\u001b[0m\n",
      "▁युरोप मा ▁केही ▁दिन यता को ▁उच्च ▁ताप क्रम सँगै ▁फैल िएको ▁ड ढे लो का ▁कारण ▁जन जीवन ▁अ स्त व्य स्त ▁बनेको ▁छ ▁।\n",
      "\n",
      "\u001b[1mbert-base-multilingual-uncased\u001b[0m\n",
      "य ##रोप ##मा कही दिन ##यता ##को उचच तापकरम ##स ##ग फल ##िएको ड ##ढ ##लो ##का कारण जन ##जी ##वन असत ##वय ##सत बन ##को छ ।\n",
      "\n",
      "\u001b[1mflax-community/roberta-hindi\u001b[0m\n",
      "à¤¯ à¥ģ à¤° à¥ĭ à¤ªà¤® à¤ ¾ Ġà¤ķ à¥ĩ à¤¹ à¥Ģ Ġà¤¦ à¤¿ à¤¨à¤¯à¤¤ à¤ ¾ à¤ķ à¥ĭ Ġà¤īà¤ļ à¥į à¤ļ Ġà¤¤ à¤ ¾ à¤ªà¤ķ à¥į à¤°à¤®à¤¸ à¤ģ à¤Ĺ à¥Ī Ġà¤« à¥Ī à¤² à¤¿ à¤ıà¤ķ à¥ĭ Ġà¤¡à¤¢ à¥ĩ à¤² à¥ĭ à¤ķ à¤ ¾ Ġà¤ķ à¤ ¾ à¤°à¤£ Ġà¤ľà¤¨à¤ľ à¥Ģ à¤µà¤¨ Ġà¤ħà¤¸ à¥į à¤¤à¤µ à¥į à¤¯à¤¸ à¥į à¤¤ Ġà¤¬à¤¨ à¥ĩ à¤ķ à¥ĭ Ġà¤Ľ Ġà¥ ¤\n",
      "\n",
      "======================================================================================================================================================\n",
      "Sentence 4: काठमाडौं महानगरपालिकाले साउन १ गतेदेखि कुहिने र नकुहिने फोहोरलाई छुट्टाछुट्टै दिनमा संकलन गर्ने भएको छ ।\n",
      "\n",
      "\u001b[1mRajan/NepaliBERT\u001b[0m\n",
      "काठमाडौ महानगरपालिका ##ल साउन [UNK] गत ##द ##खि कहि ##न र नक ##हिन फोहोरलाई छ ##ट ##टा ##छ ##ट ##ट दिनमा सक ##लन गर ##न भएको छ ।\n",
      "\n",
      "\u001b[1mamitness/nepbert\u001b[0m\n",
      "à¤ķ à¤¾ à¤łà¤® à¤¾ à¤¡ à¥Įà¤Ĥ Ġà¤®à¤¹ à¤¾ à¤¨à¤Ĺà¤°à¤ª à¤¾ à¤² à¤¿ à¤ķ à¤¾ à¤² à¥ĩ Ġà¤¸ à¤¾ à¤īà¤¨ Ġà¥§ Ġà¤Ĺà¤¤ à¥ĩ à¤¦ à¥ĩ à¤ĸ à¤¿ Ġà¤ķ à¥ģ à¤¹ à¤¿ à¤¨ à¥ĩ Ġà¤° Ġà¤¨à¤ķ à¥ģ à¤¹ à¤¿ à¤¨ à¥ĩ Ġà¤« à¥ĭ à¤¹ à¥ĭ à¤°à¤² à¤¾ à¤Ī Ġà¤Ľ à¥ģ à¤Ł à¥į à¤Ł à¤¾ à¤Ľ à¥ģ à¤Ł à¥į à¤Ł à¥Ī Ġà¤¦ à¤¿ à¤¨à¤® à¤¾ Ġà¤¸ à¤Ĥ à¤ķà¤²à¤¨ Ġà¤Ĺà¤° à¥į à¤¨ à¥ĩ Ġà¤Ńà¤ıà¤ķ à¥ĭ Ġà¤Ľ Ġà¥¤\n",
      "\n",
      "\u001b[1mSakonii/distilbert-base-nepali\u001b[0m\n",
      "▁काठमाडौं ▁महानगरपालिकाले ▁साउन ▁१ ▁गतेदेखि ▁कुहिन े ▁र ▁ न कु ह िने ▁फोहोर लाई ▁छुट्टाछुट्टै ▁दिनमा ▁संकलन ▁गर्ने ▁भएको ▁छ ▁।\n",
      "\n",
      "\u001b[1mSakonii/deberta-base-nepali\u001b[0m\n",
      "▁काठमाडौं ▁महानगरपालिकाले ▁साउन ▁१ ▁गतेदेखि ▁कुहिन े ▁र ▁ न कु ह िने ▁फोहोर लाई ▁छुट्टाछुट्टै ▁दिनमा ▁संकलन ▁गर्ने ▁भएको ▁छ ▁।\n",
      "\n",
      "\u001b[1mxlm-roberta-base\u001b[0m\n",
      "▁काठमाडौं ▁महानगरपालिका ले ▁साउन ▁१ ▁गतेदेखि ▁कु हि ने ▁र ▁न कु हि ने ▁फो हो र लाई ▁छु ट्टा छु ट्ट ै ▁दिनमा ▁संकलन ▁गर्ने ▁भएको ▁छ ▁।\n",
      "\n",
      "\u001b[1mbert-base-multilingual-uncased\u001b[0m\n",
      "का ##ठ ##मा ##ड ##ौ म ##हान ##गर ##पालिका ##ल स ##ाउन १ ग ##त ##द ##ख ##ि क ##ह ##िन र न ##क ##ह ##िन फ ##ोह ##ोर ##लाई छ ##टट ##ा ##छ ##टट दिन ##मा सकल ##न गरन भएको छ ।\n",
      "\n",
      "\u001b[1mflax-community/roberta-hindi\u001b[0m\n",
      "à¤ķ à¤ ¾ à¤łà¤® à¤ ¾ à¤¡ à¥Įà¤Ĥ Ġà¤®à¤¹ à¤ ¾ à¤¨à¤Ĺà¤°à¤ª à¤ ¾ à¤² à¤¿ à¤ķ à¤ ¾ à¤² à¥ĩ Ġà¤¸ à¤ ¾ à¤īà¤¨ Ġà¥§ Ġà¤Ĺà¤¤ à¥ĩ à¤¦ à¥ĩ à¤ĸ à¤¿ Ġà¤ķ à¥ģ à¤¹ à¤¿ à¤¨ à¥ĩ Ġà¤° Ġà¤¨à¤ķ à¥ģ à¤¹ à¤¿ à¤¨ à¥ĩ Ġà¤« à¥ĭ à¤¹ à¥ĭ à¤°à¤² à¤ ¾ à¤Ī Ġà¤Ľ à¥ģ à¤Ł à¥į à¤Ł à¤ ¾ à¤Ľ à¥ģ à¤Ł à¥į à¤Ł à¥Ī Ġà¤¦ à¤¿ à¤¨à¤® à¤ ¾ Ġà¤¸ à¤Ĥ à¤ķà¤²à¤¨ Ġà¤Ĺà¤° à¥į à¤¨ à¥ĩ Ġà¤Ńà¤ıà¤ķ à¥ĭ Ġà¤Ľ Ġà¥ ¤\n",
      "\n",
      "======================================================================================================================================================\n",
      "Sentence 5: काठमाडौंको नागार्जुनमा चितुवाको आक्रमणबाट पाँच जना घाइते भएका छन् ।\n",
      "\n",
      "\u001b[1mRajan/NepaliBERT\u001b[0m\n",
      "काठमाडौको नागा ##र ##जनमा चित ##वाको आक ##रमण ##बाट पाच जना घाइत भएका छन ।\n",
      "\n",
      "\u001b[1mamitness/nepbert\u001b[0m\n",
      "à¤ķ à¤¾ à¤łà¤® à¤¾ à¤¡ à¥Įà¤Ĥ à¤ķ à¥ĭ Ġà¤¨ à¤¾ à¤Ĺ à¤¾ à¤° à¥į à¤ľ à¥ģ à¤¨à¤® à¤¾ Ġà¤ļ à¤¿ à¤¤ à¥ģ à¤µ à¤¾ à¤ķ à¥ĭ Ġà¤Ĩà¤ķ à¥į à¤°à¤®à¤£à¤¬ à¤¾ à¤Ł Ġà¤ª à¤¾à¤ģ à¤ļ Ġà¤ľà¤¨ à¤¾ Ġà¤ĺ à¤¾ à¤ĩà¤¤ à¥ĩ Ġà¤Ńà¤ıà¤ķ à¤¾ Ġà¤Ľà¤¨ à¥į Ġà¥¤\n",
      "\n",
      "\u001b[1mSakonii/distilbert-base-nepali\u001b[0m\n",
      "▁काठमाडौंको ▁नागार्जुन मा ▁चितुवा को ▁आक्रमण बाट ▁पाँच ▁जना ▁घाइते ▁भएका ▁छन् ▁।\n",
      "\n",
      "\u001b[1mSakonii/deberta-base-nepali\u001b[0m\n",
      "▁काठमाडौंको ▁नागार्जुन मा ▁चितुवा को ▁आक्रमण बाट ▁पाँच ▁जना ▁घाइते ▁भएका ▁छन् ▁।\n",
      "\n",
      "\u001b[1mxlm-roberta-base\u001b[0m\n",
      "▁काठमाडौं को ▁ना गा र्ज ुन मा ▁ चित ुवा को ▁आक्रमण बाट ▁पाँच ▁जना ▁घाइते ▁भएका ▁छन् ▁।\n",
      "\n",
      "\u001b[1mbert-base-multilingual-uncased\u001b[0m\n",
      "का ##ठ ##मा ##ड ##ौ ##को न ##ाग ##ार ##जन ##मा च ##ित ##वा ##को आ ##करम ##ण ##बाट पाच जन ##ा घ ##ा ##इ ##त भ ##एका छन ।\n",
      "\n",
      "\u001b[1mflax-community/roberta-hindi\u001b[0m\n",
      "à¤ķ à¤ ¾ à¤łà¤® à¤ ¾ à¤¡ à¥Įà¤Ĥ à¤ķ à¥ĭ Ġà¤¨ à¤ ¾ à¤Ĺ à¤ ¾ à¤° à¥į à¤ľ à¥ģ à¤¨à¤® à¤ ¾ Ġà¤ļ à¤¿ à¤¤ à¥ģ à¤µ à¤ ¾ à¤ķ à¥ĭ Ġà¤Ĩà¤ķ à¥į à¤°à¤®à¤£à¤¬ à¤ ¾ à¤Ł Ġà¤ª à¤¾à¤ģ à¤ļ Ġà¤ľà¤¨ à¤ ¾ Ġà¤ĺ à¤ ¾ à¤ĩà¤¤ à¥ĩ Ġà¤Ńà¤ıà¤ķ à¤ ¾ Ġà¤Ľà¤¨ à¥į Ġà¥ ¤\n",
      "\n",
      "======================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all sentences\n",
    "for idx, sentence in enumerate(sentences):\n",
    "    print(f'Sentence {idx+1}: {sentence}')\n",
    "    print()\n",
    "    tokenize_sentence(sentence)\n",
    "    print('='*150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing one sentence based on type of tokenizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordPiece Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSentence:\u001b[0m काठमाडौंको नागार्जुनमा चितुवाको आक्रमणबाट पाँच जना घाइते भएका छन् ।\n",
      "\n",
      "\u001b[1mRajan/NepaliBERT\u001b[0m\n",
      "काठमाडौको नागा ##र ##जनमा चित ##वाको आक ##रमण ##बाट पाच जना घाइत भएका छन ।\n",
      "\n",
      "\u001b[1mbert-base-multilingual-uncased\u001b[0m\n",
      "का ##ठ ##मा ##ड ##ौ ##को न ##ाग ##ार ##जन ##मा च ##ित ##वा ##को आ ##करम ##ण ##बाट पाच जन ##ा घ ##ा ##इ ##त भ ##एका छन ।\n",
      "\n",
      "======================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f'{bold(\"Sentence:\")} {sentences[-1]}\\n')\n",
    "tokenize_sentence(sentence, tokenizers=wp_tokenizers)\n",
    "print('='*150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SentencePiece Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSentence:\u001b[0m काठमाडौंको नागार्जुनमा चितुवाको आक्रमणबाट पाँच जना घाइते भएका छन् ।\n",
      "\n",
      "\u001b[1mSakonii/distilbert-base-nepali\u001b[0m\n",
      "▁काठमाडौंको ▁नागार्जुन मा ▁चितुवा को ▁आक्रमण बाट ▁पाँच ▁जना ▁घाइते ▁भएका ▁छन् ▁।\n",
      "\n",
      "\u001b[1mSakonii/deberta-base-nepali\u001b[0m\n",
      "▁काठमाडौंको ▁नागार्जुन मा ▁चितुवा को ▁आक्रमण बाट ▁पाँच ▁जना ▁घाइते ▁भएका ▁छन् ▁।\n",
      "\n",
      "\u001b[1mxlm-roberta-base\u001b[0m\n",
      "▁काठमाडौं को ▁ना गा र्ज ुन मा ▁ चित ुवा को ▁आक्रमण बाट ▁पाँच ▁जना ▁घाइते ▁भएका ▁छन् ▁।\n",
      "\n",
      "======================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f'{bold(\"Sentence:\")} {sentences[-1]}\\n')\n",
    "tokenize_sentence(sentence, tokenizers=sp_tokenizers)\n",
    "print('='*150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Byte-level BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mSentence:\u001b[0m काठमाडौंको नागार्जुनमा चितुवाको आक्रमणबाट पाँच जना घाइते भएका छन् ।\n",
      "\n",
      "\u001b[1mamitness/nepbert\u001b[0m\n",
      "à¤ķ à¤¾ à¤łà¤® à¤¾ à¤¡ à¥Įà¤Ĥ à¤ķ à¥ĭ Ġà¤¨ à¤¾ à¤Ĺ à¤¾ à¤° à¥į à¤ľ à¥ģ à¤¨à¤® à¤¾ Ġà¤ļ à¤¿ à¤¤ à¥ģ à¤µ à¤¾ à¤ķ à¥ĭ Ġà¤Ĩà¤ķ à¥į à¤°à¤®à¤£à¤¬ à¤¾ à¤Ł Ġà¤ª à¤¾à¤ģ à¤ļ Ġà¤ľà¤¨ à¤¾ Ġà¤ĺ à¤¾ à¤ĩà¤¤ à¥ĩ Ġà¤Ńà¤ıà¤ķ à¤¾ Ġà¤Ľà¤¨ à¥į Ġà¥¤\n",
      "\n",
      "\u001b[1mflax-community/roberta-hindi\u001b[0m\n",
      "à¤ķ à¤ ¾ à¤łà¤® à¤ ¾ à¤¡ à¥Įà¤Ĥ à¤ķ à¥ĭ Ġà¤¨ à¤ ¾ à¤Ĺ à¤ ¾ à¤° à¥į à¤ľ à¥ģ à¤¨à¤® à¤ ¾ Ġà¤ļ à¤¿ à¤¤ à¥ģ à¤µ à¤ ¾ à¤ķ à¥ĭ Ġà¤Ĩà¤ķ à¥į à¤°à¤®à¤£à¤¬ à¤ ¾ à¤Ł Ġà¤ª à¤¾à¤ģ à¤ļ Ġà¤ľà¤¨ à¤ ¾ Ġà¤ĺ à¤ ¾ à¤ĩà¤¤ à¥ĩ Ġà¤Ńà¤ıà¤ķ à¤ ¾ Ġà¤Ľà¤¨ à¥į Ġà¥ ¤\n",
      "\n",
      "======================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(f'{bold(\"Sentence:\")} {sentences[-1]}\\n')\n",
    "tokenize_sentence(sentence, tokenizers=bp_tokenizers)\n",
    "print('='*150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_roberta = AutoModelForSequenceClassification.from_pretrained(RoBERTa_AMITNESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_roberta_tokens = list(tokenizers[4].get_vocab().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_roberta_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_roberta.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''.join(str(tokenizers[4].encode(sentences[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_roberta.base_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['म घर जाँदैगर्दा रुखबाट स्याउ खस्यो']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize all sentences\n",
    "for idx, sentence in enumerate(sentences):\n",
    "    print(f'Sentence {idx+1}: {sentence}')\n",
    "    print()\n",
    "    tokenize_sentence(sentence)\n",
    "    print('='*150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mBERT_tokenizer = tokenizers[-1]\n",
    "bert_tokenizer = tokenizers[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1= 'स्वास्थ्य तथा जनसंख्या मन्त्रालयले गत असार ९ गतेदेखि १५ गतेसम्म खोप लगाएका बालबालिकालाई आजदेखि दोस्रो मात्रा लगाउन थालेको हो ।'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sent1 = bert_tokenizer.tokenize(sent1)\n",
    "len(tokenized_sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sent1 = bert_tokenizer.encode(sent1)\n",
    "len(encoded_sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded_sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.decode(encoded_sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bert_tokenizer.convert_ids_to_tokens(encoded_sent1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.prepare_for_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".RNN",
   "language": "python",
   "name": ".rnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "3b305b80917f196789ba60f88b8ccd2c93dcce63da16b55ee72ed7db7c17015d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
