{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nepali Pretrained Tokenizers examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Suyogya\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer IDs\n",
    "BERT_SHUSHANT = 'Shushant/nepaliBERT'\n",
    "BERT_NOWALAB = 'nowalab/nepali-bert-npvec1'\n",
    "BERT_RAJAN = 'Rajan/NepaliBERT'\n",
    "DistilBERT_SAKONII = 'Sakonii/distilbert-base-nepali'\n",
    "RoBERTa_AMITNESS = 'amitness/nepbert'\n",
    "DeBERTa_SAKONII = 'Sakonii/deberta-base-nepali'\n",
    "XLM_RoBERTa_BASE = 'xlm-roberta-base'\n",
    "XLM_BERT_BASE = 'bert-base-multilingual-uncased'\n",
    "\n",
    "tokenizer_ids = [BERT_SHUSHANT, BERT_NOWALAB, BERT_RAJAN, DistilBERT_SAKONII, RoBERTa_AMITNESS, DeBERTa_SAKONII, XLM_RoBERTa_BASE, XLM_BERT_BASE]\n",
    "\n",
    "# List of Sentences\n",
    "sentences = [\n",
    "    'नेपाल आफ्नै संस्कृतिका कारण विश्वमा सबैतिर चिनिएको हो ।', \n",
    "    'स्वास्थ्य तथा जनसंख्या मन्त्रालयले गत असार ९ गतेदेखि १५ गतेसम्म खोप लगाएका बालबालिकालाई आजदेखि दोस्रो मात्रा लगाउन थालेको हो ।', \n",
    "    'युरोपमा केही दिनयताको उच्च तापक्रमसँगै फैलिएको डढेलोका कारण जनजीवन अस्तव्यस्त बनेको छ ।',\n",
    "    'काठमाडौं महानगरपालिकाले साउन १ गतेदेखि कुहिने र नकुहिने फोहोरलाई छुट्टाछुट्टै दिनमा संकलन गर्ने भएको छ ।',\n",
    "    'काठमाडौंको नागार्जुनमा चितुवाको आक्रमणबाट पाँच जना घाइते भएका छन् ।',]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Shushant/nepaliBERT ...\n",
      "Loading nowalab/nepali-bert-npvec1 ...\n",
      "Loading Rajan/NepaliBERT ...\n",
      "Loading Sakonii/distilbert-base-nepali ...\n",
      "Loading amitness/nepbert ...\n",
      "Loading Sakonii/deberta-base-nepali ...\n",
      "Loading xlm-roberta-base ...\n",
      "Loading bert-base-multilingual-uncased ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 9.35kB/s]\n",
      "Downloading: 100%|██████████| 625/625 [00:00<00:00, 206kB/s]\n",
      "Downloading: 100%|██████████| 851k/851k [00:02<00:00, 403kB/s]  \n",
      "Downloading: 100%|██████████| 1.64M/1.64M [00:03<00:00, 560kB/s] \n"
     ]
    }
   ],
   "source": [
    "tokenizers = []\n",
    "for id in tokenizer_ids:\n",
    "    print('Loading', id, '...')\n",
    "    tokenizers.append(AutoTokenizer.from_pretrained(id))\n",
    "\n",
    "def tokenize_sentence(text):\n",
    "    for idx, tokenizer in enumerate(tokenizers):\n",
    "        print('Tokenizing using', tokenizer_ids[idx])\n",
    "        print(' '.join(tokenizer.tokenize(text)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PreTrainedTokenizerFast(name_or_path='Shushant/nepaliBERT', vocab_size=30522, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "\n",
      "PreTrainedTokenizerFast(name_or_path='nowalab/nepali-bert-npvec1', vocab_size=30000, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "\n",
      "PreTrainedTokenizerFast(name_or_path='Rajan/NepaliBERT', vocab_size=50000, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "\n",
      "PreTrainedTokenizerFast(name_or_path='Sakonii/distilbert-base-nepali', vocab_size=24581, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)})\n",
      "\n",
      "PreTrainedTokenizerFast(name_or_path='amitness/nepbert', vocab_size=52000, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'sep_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'cls_token': AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=True)})\n",
      "\n",
      "PreTrainedTokenizerFast(name_or_path='Sakonii/deberta-base-nepali', vocab_size=24581, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)})\n",
      "\n",
      "PreTrainedTokenizerFast(name_or_path='xlm-roberta-base', vocab_size=250002, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'sep_token': '</s>', 'pad_token': '<pad>', 'cls_token': '<s>', 'mask_token': AddedToken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False)})\n",
      "\n",
      "PreTrainedTokenizerFast(name_or_path='bert-base-multilingual-uncased', vocab_size=105879, model_max_len=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(f'{tokenizer}\\n') for tokenizer in tokenizers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: नेपाल आफ्नै संस्कृतिका कारण विश्वमा सबैतिर चिनिएको हो ।\n",
      "\n",
      "Tokenizing using Shushant/nepaliBERT\n",
      "नपा ##ल आफ ##न सस ##कत ##िका कारण विश ##वमा सब ##तिर चिनिएको हो ।\n",
      "\n",
      "Tokenizing using nowalab/nepali-bert-npvec1\n",
      "नपा ##ल आफ ##न सस ##कत ##िका कारण [UNK] सब ##तिर चिनिए ##को हो ।\n",
      "\n",
      "Tokenizing using Rajan/NepaliBERT\n",
      "नपा ##ल आफ ##न सस ##कति ##का कारण विश ##वमा सब ##तिर चिनिएको हो ।\n",
      "\n",
      "Tokenizing using Sakonii/distilbert-base-nepali\n",
      "▁नेपाल ▁आफ्नै ▁संस्कृति का ▁कारण ▁विश्वमा ▁सबैतिर ▁चिनिए को ▁हो ▁।\n",
      "\n",
      "Tokenizing using amitness/nepbert\n",
      "à¤¨ à¥ĩ à¤ª à¤¾ à¤² Ġà¤Ĩà¤« à¥į à¤¨ à¥Ī Ġà¤¸ à¤Ĥ à¤¸ à¥į à¤ķ à¥ĥ à¤¤ à¤¿ à¤ķ à¤¾ Ġà¤ķ à¤¾ à¤°à¤£ Ġà¤µ à¤¿ à¤¶ à¥į à¤µà¤® à¤¾ Ġà¤¸à¤¬ à¥Ī à¤¤ à¤¿ à¤° Ġà¤ļ à¤¿ à¤¨ à¤¿ à¤ıà¤ķ à¥ĭ Ġà¤¹ à¥ĭ Ġà¥¤\n",
      "\n",
      "Tokenizing using Sakonii/deberta-base-nepali\n",
      "▁नेपाल ▁आफ्नै ▁संस्कृति का ▁कारण ▁विश्वमा ▁सबैतिर ▁चिनिए को ▁हो ▁।\n",
      "\n",
      "Tokenizing using xlm-roberta-base\n",
      "▁नेपाल ▁आफ्नै ▁संस्कृति का ▁कारण ▁विश्व मा ▁सबै तिर ▁चिन िएको ▁हो ▁।\n",
      "\n",
      "Tokenizing using bert-base-multilingual-uncased\n",
      "नपाल आ ##फ ##न ससकत ##िका कारण विशव ##मा सब ##ति ##र च ##िन ##िएको हो ।\n",
      "\n",
      "======================================================================================================================================================\n",
      "Sentence 2: स्वास्थ्य तथा जनसंख्या मन्त्रालयले गत असार ९ गतेदेखि १५ गतेसम्म खोप लगाएका बालबालिकालाई आजदेखि दोस्रो मात्रा लगाउन थालेको हो ।\n",
      "\n",
      "Tokenizing using Shushant/nepaliBERT\n",
      "सवा ##स ##थ ##य तथा जनस ##ख ##या मन ##तर ##ालय ##ल गत असार ९ गत ##द ##खि १५ गत ##सम ##म खोप लगाएका बालबालिकालाई आज ##द ##खि दोस ##रो मा ##तर ##ा लगाउन थाल ##को हो ।\n",
      "\n",
      "Tokenizing using nowalab/nepali-bert-npvec1\n",
      "[UNK] तथा जनस ##ख ##या मन ##तरा ##लय ##ल गत असार [UNK] गत ##द ##खि [UNK] गत ##सम ##म खोप लगाए ##का [UNK] आज ##द ##खि दोस ##रो मात ##रा लगाउन थाल ##को हो ।\n",
      "\n",
      "Tokenizing using Rajan/NepaliBERT\n",
      "सवा ##स ##थ ##य तथा जन ##स ##ख ##या मन ##तर ##ालय ##ल गत असार [UNK] गत ##द ##खि [UNK] गत ##सम ##म खोप लगाएका बालबालिकालाई आज ##द ##खि दोस ##रो मात ##रा लगाउन थाल ##को हो ।\n",
      "\n",
      "Tokenizing using Sakonii/distilbert-base-nepali\n",
      "▁स्वास्थ्य ▁तथा ▁जनसंख्या ▁मन्त्रालयले ▁गत ▁असार ▁९ ▁गतेदेखि ▁१५ ▁गतेसम्म ▁खोप ▁लगाएका ▁बालबालिकालाई ▁आजदेखि ▁दोस्रो ▁मात्रा ▁लगाउन ▁थालेको ▁हो ▁।\n",
      "\n",
      "Tokenizing using amitness/nepbert\n",
      "à¤¸ à¥į à¤µ à¤¾ à¤¸ à¥į à¤¥ à¥į à¤¯ Ġà¤¤à¤¥ à¤¾ Ġà¤ľà¤¨à¤¸ à¤Ĥ à¤ĸ à¥į à¤¯ à¤¾ Ġà¤®à¤¨ à¥į à¤¤ à¥į à¤° à¤¾ à¤²à¤¯à¤² à¥ĩ Ġà¤Ĺà¤¤ Ġà¤ħà¤¸ à¤¾ à¤° Ġà¥¯ Ġà¤Ĺà¤¤ à¥ĩ à¤¦ à¥ĩ à¤ĸ à¤¿ Ġà¥§à¥« Ġà¤Ĺà¤¤ à¥ĩ à¤¸à¤® à¥į à¤® Ġà¤ĸ à¥ĭ à¤ª Ġà¤²à¤Ĺ à¤¾ à¤ıà¤ķ à¤¾ Ġà¤¬ à¤¾ à¤²à¤¬ à¤¾ à¤² à¤¿ à¤ķ à¤¾ à¤² à¤¾ à¤Ī Ġà¤Ĩà¤ľà¤¦ à¥ĩ à¤ĸ à¤¿ Ġà¤¦ à¥ĭ à¤¸ à¥į à¤° à¥ĭ Ġà¤® à¤¾ à¤¤ à¥į à¤° à¤¾ Ġà¤²à¤Ĺ à¤¾ à¤īà¤¨ Ġà¤¥ à¤¾ à¤² à¥ĩ à¤ķ à¥ĭ Ġà¤¹ à¥ĭ Ġà¥¤\n",
      "\n",
      "Tokenizing using Sakonii/deberta-base-nepali\n",
      "▁स्वास्थ्य ▁तथा ▁जनसंख्या ▁मन्त्रालयले ▁गत ▁असार ▁९ ▁गतेदेखि ▁१५ ▁गतेसम्म ▁खोप ▁लगाएका ▁बालबालिकालाई ▁आजदेखि ▁दोस्रो ▁मात्रा ▁लगाउन ▁थालेको ▁हो ▁।\n",
      "\n",
      "Tokenizing using xlm-roberta-base\n",
      "▁स्वास्थ्य ▁तथा ▁जनसंख्या ▁मन्त्रालयले ▁गत ▁असार ▁९ ▁गतेदेखि ▁१५ ▁गतेसम्म ▁खो प ▁लगाए का ▁बालबालिका लाई ▁आज देखि ▁दोस्रो ▁मात्रा ▁लगाउन ▁थालेको ▁हो ▁।\n",
      "\n",
      "Tokenizing using bert-base-multilingual-uncased\n",
      "सवा ##स ##थय तथा जनसखया मन ##तर ##ालय ##ल ग ##त अस ##ार ९ ग ##त ##द ##ख ##ि १५ ग ##त ##सम ##म ख ##ो ##प लगा ##एका ब ##ाल ##बा ##लि ##काल ##ाई आज ##द ##ख ##ि दो ##सर ##ो मातरा लगा ##उन था ##लक ##ो हो ।\n",
      "\n",
      "======================================================================================================================================================\n",
      "Sentence 3: युरोपमा केही दिनयताको उच्च तापक्रमसँगै फैलिएको डढेलोका कारण जनजीवन अस्तव्यस्त बनेको छ ।\n",
      "\n",
      "Tokenizing using Shushant/nepaliBERT\n",
      "य ##रोप ##मा कही दिनयता ##को उच ##च ताप ##कर ##मस ##ग फल ##िएको डढ ##लोक ##ा कारण जनजीवन अस ##त ##व ##यस ##त बन ##को छ ।\n",
      "\n",
      "Tokenizing using nowalab/nepali-bert-npvec1\n",
      "य ##रोप ##मा [UNK] दिन ##यता ##को उ ##च ##च ताप ##कर ##मस ##ग फल ##िए ##को डढ ##लोक ##ा कारण [UNK] [UNK] बन ##को छ ।\n",
      "\n",
      "Tokenizing using Rajan/NepaliBERT\n",
      "य ##रोप ##मा कही दिनयता ##को उच ##च ताप ##कर ##मस ##ग फल ##िएको डढ ##लोक ##ा कारण जनजीवन अस ##त ##व ##यस ##त बन ##को छ ।\n",
      "\n",
      "Tokenizing using Sakonii/distilbert-base-nepali\n",
      "▁युरोप मा ▁केही ▁दिन यता को ▁उच्च ▁तापक्रम सँगै ▁फैलिएको ▁डढेलो का ▁कारण ▁जनजीवन ▁अस्तव्यस्त ▁बनेको ▁छ ▁।\n",
      "\n",
      "Tokenizing using amitness/nepbert\n",
      "à¤¯ à¥ģ à¤° à¥ĭ à¤ªà¤® à¤¾ Ġà¤ķ à¥ĩ à¤¹ à¥Ģ Ġà¤¦ à¤¿ à¤¨à¤¯à¤¤ à¤¾ à¤ķ à¥ĭ Ġà¤īà¤ļ à¥į à¤ļ Ġà¤¤ à¤¾ à¤ªà¤ķ à¥į à¤°à¤®à¤¸ à¤ģ à¤Ĺ à¥Ī Ġà¤« à¥Ī à¤² à¤¿ à¤ıà¤ķ à¥ĭ Ġà¤¡à¤¢ à¥ĩ à¤² à¥ĭ à¤ķ à¤¾ Ġà¤ķ à¤¾ à¤°à¤£ Ġà¤ľà¤¨à¤ľ à¥Ģ à¤µà¤¨ Ġà¤ħà¤¸ à¥į à¤¤à¤µ à¥į à¤¯à¤¸ à¥į à¤¤ Ġà¤¬à¤¨ à¥ĩ à¤ķ à¥ĭ Ġà¤Ľ Ġà¥¤\n",
      "\n",
      "Tokenizing using Sakonii/deberta-base-nepali\n",
      "▁युरोप मा ▁केही ▁दिन यता को ▁उच्च ▁तापक्रम सँगै ▁फैलिएको ▁डढेलो का ▁कारण ▁जनजीवन ▁अस्तव्यस्त ▁बनेको ▁छ ▁।\n",
      "\n",
      "Tokenizing using xlm-roberta-base\n",
      "▁युरोप मा ▁केही ▁दिन यता को ▁उच्च ▁ताप क्रम सँगै ▁फैल िएको ▁ड ढे लो का ▁कारण ▁जन जीवन ▁अ स्त व्य स्त ▁बनेको ▁छ ▁।\n",
      "\n",
      "Tokenizing using bert-base-multilingual-uncased\n",
      "य ##रोप ##मा कही दिन ##यता ##को उचच तापकरम ##स ##ग फल ##िएको ड ##ढ ##लो ##का कारण जन ##जी ##वन असत ##वय ##सत बन ##को छ ।\n",
      "\n",
      "======================================================================================================================================================\n",
      "Sentence 4: काठमाडौं महानगरपालिकाले साउन १ गतेदेखि कुहिने र नकुहिने फोहोरलाई छुट्टाछुट्टै दिनमा संकलन गर्ने भएको छ ।\n",
      "\n",
      "Tokenizing using Shushant/nepaliBERT\n",
      "काठमाडौ महानगरपालिका ##ल साउन १ गत ##द ##खि कहि ##न र नक ##हिन फोहोर ##लाई छ ##ट ##टा ##छ ##ट ##ट दिनमा सक ##लन गर ##न भएको छ ।\n",
      "\n",
      "Tokenizing using nowalab/nepali-bert-npvec1\n",
      "काठमाडौ महानगरपालिका ##ल साउन [UNK] गत ##द ##खि कहि ##न र नक ##हिन [UNK] छट ##टा ##छ ##ट ##ट दिन ##मा सकल ##न गर ##न भएको छ ।\n",
      "\n",
      "Tokenizing using Rajan/NepaliBERT\n",
      "काठमाडौ महानगरपालिका ##ल साउन [UNK] गत ##द ##खि कहि ##न र नक ##हिन फोहोरलाई छ ##ट ##टा ##छ ##ट ##ट दिनमा सक ##लन गर ##न भएको छ ।\n",
      "\n",
      "Tokenizing using Sakonii/distilbert-base-nepali\n",
      "▁काठमाडौं ▁महानगरपालिकाले ▁साउन ▁१ ▁गतेदेखि ▁कुहिन े ▁र ▁ न कु ह िने ▁फोहोर लाई ▁छुट्टाछुट्टै ▁दिनमा ▁संकलन ▁गर्ने ▁भएको ▁छ ▁।\n",
      "\n",
      "Tokenizing using amitness/nepbert\n",
      "à¤ķ à¤¾ à¤łà¤® à¤¾ à¤¡ à¥Įà¤Ĥ Ġà¤®à¤¹ à¤¾ à¤¨à¤Ĺà¤°à¤ª à¤¾ à¤² à¤¿ à¤ķ à¤¾ à¤² à¥ĩ Ġà¤¸ à¤¾ à¤īà¤¨ Ġà¥§ Ġà¤Ĺà¤¤ à¥ĩ à¤¦ à¥ĩ à¤ĸ à¤¿ Ġà¤ķ à¥ģ à¤¹ à¤¿ à¤¨ à¥ĩ Ġà¤° Ġà¤¨à¤ķ à¥ģ à¤¹ à¤¿ à¤¨ à¥ĩ Ġà¤« à¥ĭ à¤¹ à¥ĭ à¤°à¤² à¤¾ à¤Ī Ġà¤Ľ à¥ģ à¤Ł à¥į à¤Ł à¤¾ à¤Ľ à¥ģ à¤Ł à¥į à¤Ł à¥Ī Ġà¤¦ à¤¿ à¤¨à¤® à¤¾ Ġà¤¸ à¤Ĥ à¤ķà¤²à¤¨ Ġà¤Ĺà¤° à¥į à¤¨ à¥ĩ Ġà¤Ńà¤ıà¤ķ à¥ĭ Ġà¤Ľ Ġà¥¤\n",
      "\n",
      "Tokenizing using Sakonii/deberta-base-nepali\n",
      "▁काठमाडौं ▁महानगरपालिकाले ▁साउन ▁१ ▁गतेदेखि ▁कुहिन े ▁र ▁ न कु ह िने ▁फोहोर लाई ▁छुट्टाछुट्टै ▁दिनमा ▁संकलन ▁गर्ने ▁भएको ▁छ ▁।\n",
      "\n",
      "Tokenizing using xlm-roberta-base\n",
      "▁काठमाडौं ▁महानगरपालिका ले ▁साउन ▁१ ▁गतेदेखि ▁कु हि ने ▁र ▁न कु हि ने ▁फो हो र लाई ▁छु ट्टा छु ट्ट ै ▁दिनमा ▁संकलन ▁गर्ने ▁भएको ▁छ ▁।\n",
      "\n",
      "Tokenizing using bert-base-multilingual-uncased\n",
      "का ##ठ ##मा ##ड ##ौ म ##हान ##गर ##पालिका ##ल स ##ाउन १ ग ##त ##द ##ख ##ि क ##ह ##िन र न ##क ##ह ##िन फ ##ोह ##ोर ##लाई छ ##टट ##ा ##छ ##टट दिन ##मा सकल ##न गरन भएको छ ।\n",
      "\n",
      "======================================================================================================================================================\n",
      "Sentence 5: काठमाडौंको नागार्जुनमा चितुवाको आक्रमणबाट पाँच जना घाइते भएका छन् ।\n",
      "\n",
      "Tokenizing using Shushant/nepaliBERT\n",
      "काठमाडौको नागा ##र ##जनमा चितव ##ाको आक ##रमण ##बाट पाच जना घाइत भएका छन ।\n",
      "\n",
      "Tokenizing using nowalab/nepali-bert-npvec1\n",
      "काठमाडौ ##को नागा ##र ##जन ##मा [UNK] आक ##रमण ##बाट पाच जना घाइ ##त भएका छन ।\n",
      "\n",
      "Tokenizing using Rajan/NepaliBERT\n",
      "काठमाडौको नागा ##र ##जनमा चित ##वाको आक ##रमण ##बाट पाच जना घाइत भएका छन ।\n",
      "\n",
      "Tokenizing using Sakonii/distilbert-base-nepali\n",
      "▁काठमाडौंको ▁नागार्जुन मा ▁चितुवा को ▁आक्रमण बाट ▁पाँच ▁जना ▁घाइते ▁भएका ▁छन् ▁।\n",
      "\n",
      "Tokenizing using amitness/nepbert\n",
      "à¤ķ à¤¾ à¤łà¤® à¤¾ à¤¡ à¥Įà¤Ĥ à¤ķ à¥ĭ Ġà¤¨ à¤¾ à¤Ĺ à¤¾ à¤° à¥į à¤ľ à¥ģ à¤¨à¤® à¤¾ Ġà¤ļ à¤¿ à¤¤ à¥ģ à¤µ à¤¾ à¤ķ à¥ĭ Ġà¤Ĩà¤ķ à¥į à¤°à¤®à¤£à¤¬ à¤¾ à¤Ł Ġà¤ª à¤¾à¤ģ à¤ļ Ġà¤ľà¤¨ à¤¾ Ġà¤ĺ à¤¾ à¤ĩà¤¤ à¥ĩ Ġà¤Ńà¤ıà¤ķ à¤¾ Ġà¤Ľà¤¨ à¥į Ġà¥¤\n",
      "\n",
      "Tokenizing using Sakonii/deberta-base-nepali\n",
      "▁काठमाडौंको ▁नागार्जुन मा ▁चितुवा को ▁आक्रमण बाट ▁पाँच ▁जना ▁घाइते ▁भएका ▁छन् ▁।\n",
      "\n",
      "Tokenizing using xlm-roberta-base\n",
      "▁काठमाडौं को ▁ना गा र्ज ुन मा ▁ चित ुवा को ▁आक्रमण बाट ▁पाँच ▁जना ▁घाइते ▁भएका ▁छन् ▁।\n",
      "\n",
      "Tokenizing using bert-base-multilingual-uncased\n",
      "का ##ठ ##मा ##ड ##ौ ##को न ##ाग ##ार ##जन ##मा च ##ित ##वा ##को आ ##करम ##ण ##बाट पाच जन ##ा घ ##ा ##इ ##त भ ##एका छन ।\n",
      "\n",
      "======================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all sentences\n",
    "for idx, sentence in enumerate(sentences):\n",
    "    print(f'Sentence {idx+1}: {sentence}')\n",
    "    print()\n",
    "    tokenize_sentence(sentence)\n",
    "    print('='*150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenize_sentence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\nlp-research\\ait-research\\code\\nepali-pretrained-tokenizers.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/nlp-research/ait-research/code/nepali-pretrained-tokenizers.ipynb#ch0000006?line=0'>1</a>\u001b[0m tokenize_sentence(TEXT_1)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenize_sentence' is not defined"
     ]
    }
   ],
   "source": [
    "tokenize_sentence(TEXT_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 466/466 [00:00<00:00, 155kB/s]\n",
      "Downloading: 100%|██████████| 527k/527k [00:02<00:00, 255kB/s]  \n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"nowalab/nepali-bert-npvec1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[UNK] तथा जनस ##ख ##या मन ##तरा ##लय ##ल गत असार [UNK] गत ##द ##खि [UNK] गत ##सम ##म खोप लगाए ##का [UNK] आज ##द ##खि दोस ##रो मात ##रा लगाउन थाल ##को हो ।'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(tokenizer.tokenize(TEXT_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 589/589 [00:00<00:00, 589kB/s]\n",
      "Downloading: 100%|██████████| 516k/516k [00:02<00:00, 206kB/s]  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'सवा ##स ##थ ##य तथा जनस ##ख ##या मन ##तर ##ालय ##ल गत असार ९ गत ##द ##खि १५ गत ##सम ##म खोप लगाएका बालबालिकालाई आज ##द ##खि दोस ##रो मा ##तर ##ा लगाउन थाल ##को हो ।'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(BERT_SHUSHANT)\n",
    "' '.join(tokenizer.tokenize(TEXT_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at amitness/nepbert were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at amitness/nepbert and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "np_roberta = AutoModelForSequenceClassification.from_pretrained(RoBERTa_AMITNESS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_roberta_tokens = list(tokenizers[4].get_vocab().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_roberta_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(52000, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"_name_or_path\": \"amitness/nepbert\",\n",
       "  \"architectures\": [\n",
       "    \"RobertaForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"classifier_dropout\": null,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 514,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"position_embedding_type\": \"absolute\",\n",
       "  \"transformers_version\": \"4.20.1\",\n",
       "  \"type_vocab_size\": 1,\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 52000\n",
       "}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_roberta.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[0, 267, 270, 281, 264, 272, 409, 265, 267, 290, 279, 308, 278, 265, 268, 350, 274, 269, 268, 264, 286, 264, 358, 304, 269, 307, 265, 620, 264, 455, 290, 274, 269, 266, 336, 269, 267, 269, 314, 271, 302, 271, 296, 2]'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(str(tokenizers[4].encode(sentences[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(52000, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np_roberta.base_mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['म घर जाँदैगर्दा रुखबाट स्याउ खस्यो']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: म घर जाँदैगर्दा रुखबाट स्याउ खस्यो\n",
      "\n",
      "Tokenizing using Shushant/nepaliBERT\n",
      "म घर जा ##द ##गर ##दा र ##ख ##बाट सय ##ाउ खस ##यो\n",
      "\n",
      "Tokenizing using nowalab/nepali-bert-npvec1\n",
      "म घर जा ##द ##गर ##दा रख ##बाट सय ##ाउ खस ##यो\n",
      "\n",
      "Tokenizing using Rajan/NepaliBERT\n",
      "म घर जा ##द ##गर ##दा रख ##बाट सय ##ाउ खस ##यो\n",
      "\n",
      "Tokenizing using Sakonii/distilbert-base-nepali\n",
      "▁म ▁घर ▁जाँदै गर्दा ▁रुख बाट ▁स्याउ ▁खस ्यो\n",
      "\n",
      "Tokenizing using amitness/nepbert\n",
      "à¤® Ġà¤ĺà¤° Ġà¤ľ à¤¾à¤ģ à¤¦ à¥Ī à¤Ĺà¤° à¥į à¤¦ à¤¾ Ġà¤° à¥ģ à¤ĸà¤¬ à¤¾ à¤Ł Ġà¤¸ à¥į à¤¯ à¤¾ à¤ī Ġà¤ĸà¤¸ à¥į à¤¯ à¥ĭ\n",
      "\n",
      "Tokenizing using Sakonii/deberta-base-nepali\n",
      "▁म ▁घर ▁जाँदै गर्दा ▁रुख बाट ▁स्याउ ▁खस ्यो\n",
      "\n",
      "Tokenizing using xlm-roberta-base\n",
      "▁म ▁घर ▁जा ँदै गर ्दा ▁रुख बाट ▁स्या उ ▁खस ्यो\n",
      "\n",
      "Tokenizing using bert-base-multilingual-uncased\n",
      "म घर जा ##द ##गर ##दा र ##ख ##बाट स ##या ##उ ख ##स ##यो\n",
      "\n",
      "======================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Tokenize all sentences\n",
    "for idx, sentence in enumerate(sentences):\n",
    "    print(f'Sentence {idx+1}: {sentence}')\n",
    "    print()\n",
    "    tokenize_sentence(sentence)\n",
    "    print('='*150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mBERT_tokenizer = tokenizers[-1]\n",
    "bert_tokenizer = tokenizers[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1= 'स्वास्थ्य तथा जनसंख्या मन्त्रालयले गत असार ९ गतेदेखि १५ गतेसम्म खोप लगाएका बालबालिकालाई आजदेखि दोस्रो मात्रा लगाउन थालेको हो ।'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sent1 = bert_tokenizer.tokenize(sent1)\n",
    "len(tokenized_sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['सवा', '##स', '##थ', '##य', 'तथा', 'जन', '##स', '##ख', '##या', 'मन', '##तर', '##ालय', '##ल', 'गत', 'असार', '[UNK]', 'गत', '##द', '##खि', '[UNK]', 'गत', '##सम', '##म', 'खोप', 'लगाएका', 'बालबालिकालाई', 'आज', '##द', '##खि', 'दोस', '##रो', 'मात', '##रा', 'लगाउन', 'थाल', '##को', 'हो', '।']\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_sent1 = bert_tokenizer.encode(sent1)\n",
    "len(encoded_sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 13050, 326, 356, 320, 736, 829, 326, 336, 738, 1184, 6019, 683, 315, 964, 2910, 1, 964, 335, 29703, 1, 964, 991, 314, 10589, 4711, 5980, 1334, 335, 29703, 18279, 723, 32093, 555, 3531, 1968, 518, 589, 163, 3]\n"
     ]
    }
   ],
   "source": [
    "print(encoded_sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] सवासथय तथा जनसखया मनतरालयल गत असार [UNK] गतदखि [UNK] गतसमम खोप लगाएका बालबालिकालाई आजदखि दोसरो मातरा लगाउन थालको हो । [SEP]'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.decode(encoded_sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'सवा', '##स', '##थ', '##य', 'तथा', 'जन', '##स', '##ख', '##या', 'मन', '##तर', '##ालय', '##ल', 'गत', 'असार', '[UNK]', 'गत', '##द', '##खि', '[UNK]', 'गत', '##सम', '##म', 'खोप', 'लगाएका', 'बालबालिकालाई', 'आज', '##द', '##खि', 'दोस', '##रो', 'मात', '##रा', 'लगाउन', 'थाल', '##को', 'हो', '।', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "print(bert_tokenizer.convert_ids_to_tokens(encoded_sent1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer.prepare_for_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b305b80917f196789ba60f88b8ccd2c93dcce63da16b55ee72ed7db7c17015d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
